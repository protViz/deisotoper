%\documentclass{article}
\documentclass[a4paper, 12pt, DIV15, twoside, BCOR=10.25mm]{scrreprt}

\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{rotating}

\begin{document}
\SweaveOpts{concordance=TRUE}

\title{Practical Semester Report} 
%Implementation, development, and evaluation of algorithms for the detection and extraction of isotope pattern of a mass spectrometric measurement%

\author{Lucas Schmidt}
\maketitle

\begin{abstract}
In mass spectrometry it is important to have a minimum of noise and other confounders, therefore there are methods like deisotoping to get improve the results of a measurement.  
In this report, the acknowledgment from the practical semester will be shown and likewise, the overcome project will be displayed.
The content of the practical semester was, in short, to develop an R package for deisotoping using R and Java combined together. R was used as the low-level interface to connect it with the high-level Java algorithm. 
R was used as the low-level interface to connect it with the high-level Java algorithm. The difficulties were that R and Java don’t share the same data types and also the algorithm, which was from the research paper Features-Based Deisotoping Method for Tandem Mass Spectra, wasn’t easy to implement on the Java side. We tried to implement the algorithm exactly as described in the paper, but in some places, it was not exactly clear, what the research team from the paper wanted to explain.
For our approach, we used the algorithm from the paper and reimplemented it in Java.
The results were compared to the Proteome Discoverer 2.1 about XXX percent better. But it depends very much on the configuration of our deisotoper.
The research team from the paper stated, that the results were better than the results from YADA and MsDeconv. Our implementation of the algorithm from the paper was also better than YADA, MsDeconv and the Proteome Discoverer 2.1.
\end{abstract}

\newpage
\tableofcontents
\newpage
%%%


\part{Introduction}


\chapter{The Functional Genomics Center Zurich}
The Functional Genomics Center Zurich (FGCZ) is a institution of the ETH Zurich and University Zurich specialized in research and training. The department was founded in 2002 and is since then a expert in support for Omics research supported by the latest technologies. The FGCZ collaborates with Zurich Life Sience research community. There are about 40 staff members employed, about six member are from the subject area of computer science and about ten of them are specialized in the area of bioinformations. The task of the FGCZ is in general a research and training facility with several departments. Therefor the FGCZ is composed of six specialized areas (Genomics/Transcriptomics, Proteomics \& Protein Analysis, Metabolomics \& Biophysics, Data Integration \& IT, Reserach Collaborations and Governance \& Management).

The Proteomics area is specialized on resolving the entire content of proteins in cells, organs or organisms, including protein abundances protein modifications and protein-protein interactions. The difference between organs or cell types of protein content is a challenge, also the time and physical conditions of protein content is changing, which makes the research in this region not as easy as it seems. More and more new instrumentations and data analysis tools are developed for mass spectrometry to study complex molecular and cellular process in cells and organisms.

The Genomics area works with applications and workflows, which are offered at the FGCZ to cover a large spectrum of the experiments designed to employ DNA as analyte for Next Generation Sequencing experiments. Unfortunately the FGCZ has not the capability to perform DNA and RNA extraction in its own laboratories, there are users which bring DNA and RNA samples on which the FGCZ workflows will start by preparing specific sequencing libraries. 

The Transcriptomics area researches on RNA sequencing experiments to profile the transcriptomes of several organisms to identify genes that discriminate different conditions, treatment or time points. Some small RNA samples are also often investigated.

The Metabolomics area is one of the key disciplines of systems biology. This area is specialized on research based on the composition as well as the dynamics of the metabolome in relation to genetic and environmental conditions. 

The Bioinformatics is responsible for Next Generation Sequencing and Mass Spectrometry data analysis, integration of the Omics areas, high performance computing, software development and many more.

\url{http://www.fgcz.ch/the-center/_jcr_content/par/fullwidthimage/image.imageformat.lightbox.1160524864.png/}

\chapter{Environment}
Data is generated by 20 mass spectrometry devices and 5 DNA sequencer, working nearly 24/7. The data management system is called B-Fabric, which manages 500TB of biological data in 3000 projects and over 5600 user. Besides of the standard office PCs with Microsoft Windows, Linux and Mac OS X, there is a cluster of 8000 cores available for data analysis. For my project I used a development environment on Linux using Java 8, R and Python.

\begin{itemize}
\item 20 Mass spec devices, 5 sequencer
\item 1000 active nodes windows linux mac os x
\item firewallwall uzh
\item storage size 500TB (TODO(MOS))
\item data managment system called b-fabric \cite{bfabric} ...
\item dev on Linux Box using Java 8 ....
\end{itemize}

\chapter{My Project Tasks}

Implementation of deisotoper .... see next chapter

in the follwong sections

\part{Deisotoping}
\chapter{Problem Definition}

\section{Chemical view - What is an isotope pattern?}
We are talking about stable isotopes.

Isotope example: C-12, C-13 and C-14. All of them have different probabilities of occurrence: C-12 98.9%, C-13 1.1% C-14 0.00001%.

If we would measure C in a mass spectrometer we would see 3 peaks with the mass: 12 u, 13.003355u and 14.003241u with peak heights corresponding to the relative abundances. The task of deisotoping is determining the monoisotopic mass. The monoisotopic mass could be described like: "For a given compound the monoisotopic mass is the mass of the isotopic peak whose elemental composition is composed of the most abundant isotopes of those elements." http://www.ionsource.com/tutorial/isotopes/slide8.htm
In case of C it would be 12u, which is trivial. Having N in the sample makes this task already complicated: 
N14 14.0031u 0.99636
N15 15.0001u 0.00364

Since, we measuring molecules composed of many atoms (e.g. amino acids and peptides) the isotopic pattern gets more complex: More isotope peaks can be observed.

\section{Modeling isotopic pattern}
One task is to simulate isotope patterns, see for instance:
\begin{itemize}
\item http://www.sisweb.com/mstools/isotope.html
\item http://prospector.ucsf.edu/prospector/html/instruct/isoman.html
\end{itemize}

These algorithms are usually based on the binomial theorem. Quite advanced algorithms to speed up the computation of theoretical isotope patterns for large molecules exist.
Since peptides have a similar composition there exists also the notion of an "averagine mass" and averagine isotope pattern.

Averagine Mass - An averagine mass gives you a typical profile that you would obtain for a given mass charge combination assuming that the peptide is made up of "average" amino acids. it is most useful for fitting profiles against unknown spectra. The elemental composition used for averagine was: C 4.9384 H 7.7583 N 1.3577 O 1.4773 S 0.0417.

See also:
https://www.molgen.mpg.de/88044/schulztrieglaff.pdf

\section{Deconvoluting isotopic patterns – Deisotoping}
The other task is, given an experimentally determined isotope pattern to infer to monoisotopic mass. There are also approaches to infer the exact chemical formula. 
See as reference article by Sebastian Boecker: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4321342/

But these attempts are only made for small molecules, metabolites. to achieve this, tools to propose chemical formulas given a monoisotopic mass exist, in example: 
\begin{itemize}
\item Decomp: https://bibiserv.cebitec.uni-bielefeld.de/decomp/
\item Many more software tools written in java can be found here: https://bio.informatik.uni-jena.de/software/
\end{itemize}

\section{General deisotoper algorithm}
What is the input?
What does it?
What is the output?

\section{Characteristics of Deisotoping}
\begin{itemize}
\item deisotoper
\begin{itemize}
\item intact protein - large mass, very high charge. It is difficult to determine the charge of isotope pattern in example is it 60 or 61
\item small molecules (including peptides). Easy to determine the charge of the molecule. Only few charges possible 2,3,4,5. Problem is, overlapping of isotope patterns
\item deisotoping can be applied to spectra and feature maps (2 signals)
\item feature maps have additional information like retention time, shape, which can be used to assemble signals into isotope patterns and make deisotoping easier.
\end{itemize}
\item  implementations
\begin{itemize}
\item programming language
\item license
\item is source code available, is implementation available
\item What are the input and output formats (mgf, mzXML, mzML etc)
\item stability and performance
\end{itemize}
\item Algorithm
\begin{itemize}
\item inputs (e.g. raw or already picked spectra, resolution, parent ion mass, charge state)
\item graph based, model fitting, heuristics.
\item time complexity
\item properties ? f(ms) = f(f(ms))
\end{itemize}
\item How performance is being evaluated
\begin{itemize}
\item increase identification rate
\item simulated data
\item gold standards
\end{itemize}
\end{itemize}

Specification for a new implementation
\begin{itemize}
\item Programming language Java
\item R Interface https://CRAN.R-project.org/package=rJava
\item Open source GPLv3 available through GitHub
\end{itemize}

\begin{sidewaystable}[ht]
%    \begin{tabular}{ | l | l | l | l |}
%    \hline
%    Deisotoper & Available implementations & Algorithm & Performance \\ \hline
%    Test Deisotoper & Many implementations & X Algorithm & Test Performance \\ \hline
%    \end{tabular}


%\begin{table}
\resizebox{\textwidth}{!}{
\begin{tabular}{ll|rr|lrrr|lr|rl|rr}
\hline
\hline
map     &       statistical value       &       \#regions       &       area err        &       meta&   pop size        &       mutrate &       \#gen   & compute hardware      &       cores   &       proc time       &       unit    &       &       Fig\\
\hline
US state level &  \#electors    &       49      &       0.36    &       GA      &       unknown &       unknown &       10      &       Intel Xeon CPU @ 1.5 GHz        &       1       &       $\approx$1      &       min     &               &       \ref{figure:USelection}\\
US state level &  area  &       50      &       0.17    &       -       &       -&      -&      -       &       -       &       1       &       $\ll$1  &       sec     &               &       \ref{figure:usage}\\
8x8 checkerborad        &       1:4     &       64      &       0.27    &       GA      &       256     &       0.25    &     533       &       3 GHz Intel Core i7     &       4       &       $\approx$1      &       min   &         &       \ref{figure:cmp_GA_GRASP}\\
8x8 checkerborad        &       1:4     &       64      &       0.27    &       GRASP   &       NA      &       NA      &       NA      &       3 GHz Intel Core i7     &       4       &       $\approx$1      &       min     &               &       \ref{figure:cmp_GA_GRASP}\\
\hline
US state level  &       population 1977 &       48      &      0.41     &       GA      &       300     &       0.25    &       11      &       3 GHz Intel Core i7     &       4       &       5       &       sec     &               &       \ref{figure:x77}\\
US state level  &       murder 1977     &       48      &      0.37     &       GA      &       300     &       0.25    &       20      &       3 GHz Intel Core i7     &       4       &       10      &       sec     &               &       \ref{figure:x77}\\
US state level  &       income 1977     &       48      &      0.29     &       GA      &       300     &       0.25    &       30      &       3 GHz Intel Core i7     &       4       &       16      &       sec     &               &       \ref{figure:x77}\\
US state level  &       illiteracy 1977     &   48      &      0.40     &       GA      &       300     &       0.25    &       30      &       3 GHz Intel Core i7     &       4       &       16      &       sec     &               &       \ref{figure:x77}\\
\hline
US California   &       population 2010 &       48      &       0.62    &       GA      &       240     &       0.25    &       27      &       3 GHz Intel Core i7     &       4       &       10      &       sec     &               &       \ref{figure:us_census}\\
US Colorado     &       population 2010 &       64      &       0.68    &       GA      &       320     &       0.25    &       102     &       3 GHz Intel Core i7     &       4       &       74      &       sec     &               &       \ref{figure:us_census}\\
US Florida      &       population 2010 &       68      &       0.44    &       GA      &       240     &       0.25    &       49      &       3 GHz Intel Core i7     &       4       &       39      &       sec     &               &       \ref{figure:us_census}\\
US New Jersey   &       population 2010 &       21      &       0.38    &       GA      &       105     &       0.25    &       30      &       3 GHz Intel Core i7     &       4       &       4       &       sec     &               &       \ref{figure:us_census}\\
US New York     &       population 2010 &       62      &       0.62      &     GA      &       310     &       0.25    &       148     &       3 GHz Intel Core i7     &       4       &       106     &       sec     &               &       \ref{figure:us_census}\\
\hline
Switzerland     &       population 2010 &       2300    &       0.59    &       GA      &       1280    &       0.35    &       300     &       Intel Xeon CPU E5-2698 v3 @ 2.30GHz     &       64      &       3       &       days    &               &       \ref{figure:Switzerland:population}\\
Swiss SBB railway       &       passenger frequency     &       724     &       NA      &      GA       &        1000   &       0.25    &  1000         &       Intel Xeon CPU E5-2698 v3 @ 2.30GHz     &       64      &       2       &       days    &               &       \ref{figure:sbb}\\
UK      &       electorates     &       370     &       0.57    &       GA      &       1200    &       0.25    &       4000    &       Intel Xeon CPU E5-2698 v3 @ 2.30GHz     &       64      &       3       &       days    &               &       \ref{fig:brexit}\\
\hline
\hline
\end{tabular}}
\caption{bla bla}
\end{sidewaystable}



\chapter{Related Work}

\section{Start point - Literature}
\begin{itemize}
\item http://www.hindawi.com/journals/abi/2011/210805/
\item http://www.mdpi.com/2218-1989/6/4/37/html
\item https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btm198
\end{itemize}

R> library(protViz)
Package 'protViz' version 0.2.37
R> ?deisotoper

\section{Start point - Literature}
\begin{itemize}
\item ms2spectrumprocessor http://ms.imp.ac.at/?goto=ms2spectrumprocessor PD node from Karl Maechtler.
\item MS-Deconv http://bix.ucsd.edu/projects/msdeconv/
\item YADA http://fields.scripps.edu/downloadfile2.php&filename=YADA_Setup.exe&id=22
\item NITPICK https://hci.iwr.uni-heidelberg.de/hci/softwares/nitpick
\item DeconEngineV2 https://github.com/PNNL-Comp-Mass-Spec/DeconEngineV2
\item MS_DEISOTOPE https://github.com/mobiusklein/ms_deisotope
\item mspy https://github.com/wolski/mspy see obj_peaklist.py class for implementation of simple peakpicker
\item OpenMS https://github.com/OpenMS/OpenMS see class deisotopeAndSingleChargeMSSpectrum SimpleSearchEngine.cpp for implemenation of simple peak picker
\item The Chemistry Development Kit (CDK) http://pubs.acs.org/doi/abs/10.1021/ci025584y
\end{itemize}

\chapter{Implementation}
Java8 gSON, jGraphT, 

make and maven

Rpackage, ...

Test Driven Development (TDD)

Unit Test

\chapter{Test Data}
For the project it was necessary to have sample data to test the implemented algorithm with it. For this purpose we used 20161010_04_TP_HeLa_200ng.mgf, 20161201_11_Mix1a_1_DDA.mgf, 20161201_14_Mix1b_1_DDA.mgf, 20161201_20_Mix1a_2_DDA.mgf and 20161201_22_Mix1b_2_DDA.mgf files. The files were read in by converting them to the Rdata-format and reading them into R or using a self written Java mgf reader which converted the data into our MassSpectrometryMeasurement object, which we used to perform the deisotoping on. In the table below there is some data for each mgf-file.

\begin{center}
    \begin{tabular}{ | l | l | l | }
    \hline
    File name & Name & Number of mass spetra \\ \hline
    20161010\_04\_TP\_HeLa\_200ng.mgf & TP\_HeLa\_200ng & 27583 \\ \hline
    20161201\_11\_Mix1a\_1\_DDA.mgf & Mix1a\_1\_DDA & 34335 \\ \hline
    20161201\_14\_Mix1b\_1\_DDA.mgf & Mix1b\_1\_DDA & 34751 \\ \hline
    20161201\_20\_Mix1a\_2\_DDA.mgf & Mix1a\_2\_DDA & 34433 \\ \hline
    20161201\_22\_Mix1b\_2\_DDA.mgf & Mix1b\_2\_DDA & 34467 \\ \hline
    \end{tabular}
\end{center}


\chapter{Evaluation}
\section{Clustering}
The algorithm, which we implemented in Java, uses a method to cluster peaks from one mass spectrum into isotopic sets and this isotopic sets into isotopic clusters.
For example we took the test data (TP_HeLa_200ng) and processed it through the algorithm. The test data has 27583 mass spectra, 884347 isotopic sets and 1258935 isotopic clusters. In the isotopic clusters there are about 2680555 peaks. With some computation this means, that each mass spectrum has about 32.1 isotopic sets, each isotopic set has about 45.6 isotopic clusters. Therefore there are 1.4 isotopic clusters per isotopic set, 3 peaks per isotopic set and 2.1 peaks per isotopic cluster. That means about 38.2\% of the peaks in the whole mass spectrometry measurement are in clusters. The problem with the scoring is, that the scoring only affects larger isotopic cluster graphs (isotopic set = isotopic cluster graph), therefore when the average isotopic cluster graph has 1.4 clusters, the best path which is calculated only affects about 90\% of the mass spectra.

INSERT TABLE HERE

\section{Scoring}
In the algorithm there is a part about scoring. In this part the isotopic cluster graph gets a score for each edge. The score is calculated from the isotopic cluster which is connected with the edge and each peak in a isotopic cluster is getting a score. Therefore we are interested in what the maximum of score could be. There are five scoring functions and each of it has a own multiplier. The standard configuration for the functions is 0.8 for the first, 0.5 for the second and 0.1 for the third, fourth and fifth function. Assumed we have about 250 peaks in each mass spectrum, then for the calculation of one edge we would run each scoring function about 750 in maximum. For the first scoring function the absolute maximum is the number of amino acids from the properties file, in standard about 20, therefore the maximum is 20. The second, third and fourth only can return values between 0 and 1. And the last and fifth scoring function has a estimated maximum of 6. The maximum score of an edge, when every scoring function returns it’s maximum value, is about 12975 (the sum of the maximums of the five scoring functions multiplied with 750 (average peaks of the mass spectrum multiplied with a maximum peak count of a isotopic cluster)) with the multipliers in the score function and 21750 without the multipliers. In reality the first score functions maximum is on average about 0-3 and that means, that it only looks like the first function has much impact but it hasn’t. 

INSERT TABLE HERE

\section{Aggregation}
After clustering and scoring the possible isotopic clusters, they were drawn into a graph to find the best path (path with the highest score) between the various isotopic clusters. After the best path was found, the isotopic clusters got aggregated, mostly we used the aggregation method (first), where we made the sum from the intensities for each peak from the isotopic cluster and deleted the second and the third peak and set the intensity from the first peak of the isotopic cluster to the calculated sum. To gain more information and to check if the calculation and aggregation is performed in the right way, we created a log, in which is written down when and what isotopic clusters are created and what isotopic clusters are aggregated.
Some cutouts from the log, which belongs to the TP_HeLa_200ng-file, are shown below, to explain how the aggregation works and to check for correctness in the isotopic cluster creation process.

INSERT TABLE HERE

\section{Searching}
With the Mascot Database search engine, we compared the results from our deisotoping method and the original data. Also we compared them both and other deconvolution results. The search was performed in the Mascot Daemon. For the Mascot-Search we used a fixed configuration (parameter-file), which is shown in the table below and in the screenshot of the Mascot Daemon Parameter Editor.

INSERT TABLE HERE

\chapter{Conclusion}
The whole problem with the algorithm is, that about 85\% of the runtime is needed by the score part but this part only changes about 90\% of the data and hasn’t a deep impact in the whole deisotoping procedure. The isotopic sets and isotopic clusters are the real important parts, because the clustering is the basis for the aggregation which follows later on. This happens because the score only influences the best path in each isotopic cluster graph and assumed that certainly about 90\% of the isotopic cluster graphs consist of 1 to 3 clusters and therefore 3 peaks, the impact of the score function is going against zero, because the best path is approximately the same for these small isotopic cluster graphs and therefore, no matter what scores the edges have, the isotopic clusters are aggregated the same way.
One way to improve the algorithm results is to change the error tolerance of the scoring functions, but as mentioned above this doesn’t have much impact, or to increase or decrease the delta which is used in the isotopic cluster and isotopic set generation. An other way to improve something would be at the aggregation functions or improvements at the deconvolution process, as maybe eliminating the noise peaks under a certain level of intensity or other pattern.

\begin{Schunk}
\begin{Sinput}
> library(deisotoper)
> Test
\end{Sinput}
\end{Schunk}

\part{System Administration}
%\appendix{dd}

\begin{appendix}
\chapter{R package documentation}
\chapter{LOG of the 95 days}
\end{appendix}
\end{document}


